{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Run when initialise the code\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from osmnx import io\n",
    "\n",
    "from pandas import DataFrame\n",
    "project_crs = 'epsg:3857'\n",
    "from sklearn.cluster import DBSCAN\n",
    "from shapely.geometry import  Point, LineString, MultiPolygon, MultiPoint\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from momepy import remove_false_nodes,extend_lines\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "import ast # to convert str with list to list of string\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from math import log2\n",
    "def get_five_largest_streets(simplified_network:GeoDataFrame,name_col:str,path:str):\n",
    "    \"\"\"\n",
    "    FOR TEST PURPOSES - get the five largest streets\n",
    "    :param simplified_network:\n",
    "    :param name_col: the column that stores the street name\n",
    "    :param path: the file name to store the longest street\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Dissolve the GeoDataFrame based on 'street_id' to aggregate polylines of the same street\n",
    "    dissolved_gdf = simplified_network[simplified_network['is_simplified']==1].dissolve(by=name_col)\n",
    "    dissolved_gdf['street_length'] = dissolved_gdf['geometry'].length\n",
    "    dissolved_gdf.sort_values(by='street_length',ascending=False).head(5).to_file(f'{path}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color: Green;font-size: 30px\">Module 1:Preliminary work</span>\n",
    "<ul> <li>Download data from OpenStreetMap, project it, and convert it to a GeoDataFrame. OSMnx automatically resolves topology errors and retrieves only the street-related polylines.</li>\n",
    " <li>Identify roundabout elements, if any exist, and store them in a separate DataFrame.</li>\n",
    "  <li>Remove additional irrelevant line objects based on values of the OSM 'tunnel' and 'highway' keys.</li>\n",
    "   <li>Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.</li>\n",
    "   </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color: Red;font-size: 30px\">Module 2 -3:Detect  parallel streets segments and merge them </span>\n",
    "<ul> <li>For each group of streets with the same name search for parallel segments</li>\n",
    " <li> Use DBSCAN to classify streets based on their angle, and group each class. Outliers could not consider parallel with any street, thus removed</li>\n",
    "  <li>The parallel test is on street segments that  have the same name and belong to the same angle group.\n",
    "    <ul><li>Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.</li></ul>\n",
    "    </li>\n",
    "\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turin\n"
     ]
    },
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Achituv\\\\My_Drive\\\\current_projects\\\\SOD\\\\Code/places/Turin_test'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run when initialise the code\n",
    "# In this example, the data is extracted from OSM by specifying a location's name, but you can also download data using a specified polygon. The code is designed to handle multiple polygons or location names seamlessly.\n",
    "\n",
    "# Download data from OpenStreetMap, project it, and convert it to a GeoDataFrame. OSMnx automatically resolves topology errors and retrieves only the street-related polylines.\n",
    "\n",
    "place = 'Turin'\n",
    "print(place)\n",
    "data_folder  = f'{pjr_loc}/places/{place.replace(\",\",\"_\").replace(\" \",\"_\")}_test'\n",
    "os.makedirs(f'{data_folder}/delete_2_nodes',exist_ok = True)\n",
    "os.makedirs(f'{data_folder}/split_tp_intersection',exist_ok = True)\n",
    "smplfcton_fldr = f'{data_folder}/simplification'\n",
    "data_folder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m     useful_tags_path \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname:en\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhighway\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbearing\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtunnel\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjunction\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      4\u001B[0m     ox\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mconfig(useful_tags_way\u001B[38;5;241m=\u001B[39museful_tags_path)\n\u001B[1;32m----> 5\u001B[0m graph \u001B[38;5;241m=\u001B[39m \u001B[43mox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_from_place\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnetwork_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mall\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m graph \u001B[38;5;241m=\u001B[39m ox\u001B[38;5;241m.\u001B[39mbearing\u001B[38;5;241m.\u001B[39madd_edge_bearings(graph, precision\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      7\u001B[0m graph_pro \u001B[38;5;241m=\u001B[39m ox\u001B[38;5;241m.\u001B[39mprojection\u001B[38;5;241m.\u001B[39mproject_graph(graph, to_crs\u001B[38;5;241m=\u001B[39mproject_crs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\osmnx\\graph.py:351\u001B[0m, in \u001B[0;36mgraph_from_place\u001B[1;34m(query, network_type, simplify, retain_all, truncate_by_edge, which_result, buffer_dist, clean_periphery, custom_filter)\u001B[0m\n\u001B[0;32m    348\u001B[0m utils\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConstructed place geometry polygon(s) to query API\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# create graph using this polygon(s) geometry\u001B[39;00m\n\u001B[1;32m--> 351\u001B[0m G \u001B[38;5;241m=\u001B[39m \u001B[43mgraph_from_polygon\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpolygon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnetwork_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnetwork_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43msimplify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msimplify\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretain_all\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncate_by_edge\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncate_by_edge\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclean_periphery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_periphery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_filter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_filter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    361\u001B[0m utils\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgraph_from_place returned graph with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(G)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m nodes and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(G\u001B[38;5;241m.\u001B[39medges)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m edges\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m G\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\osmnx\\graph.py:436\u001B[0m, in \u001B[0;36mgraph_from_polygon\u001B[1;34m(polygon, network_type, simplify, retain_all, truncate_by_edge, clean_periphery, custom_filter)\u001B[0m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;66;03m# create buffered graph from the downloaded data\u001B[39;00m\n\u001B[0;32m    435\u001B[0m bidirectional \u001B[38;5;241m=\u001B[39m network_type \u001B[38;5;129;01min\u001B[39;00m settings\u001B[38;5;241m.\u001B[39mbidirectional_network_types\n\u001B[1;32m--> 436\u001B[0m G_buff \u001B[38;5;241m=\u001B[39m \u001B[43m_create_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse_jsons\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbidirectional\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;66;03m# truncate buffered graph to the buffered polygon and retain_all for\u001B[39;00m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;66;03m# now. needed because overpass returns entire ways that also include\u001B[39;00m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;66;03m# nodes outside the poly if the way (that is, a way with a single OSM\u001B[39;00m\n\u001B[0;32m    441\u001B[0m \u001B[38;5;66;03m# ID) has a node inside the poly at some point.\u001B[39;00m\n\u001B[0;32m    442\u001B[0m G_buff \u001B[38;5;241m=\u001B[39m truncate\u001B[38;5;241m.\u001B[39mtruncate_graph_polygon(G_buff, poly_buff, \u001B[38;5;28;01mTrue\u001B[39;00m, truncate_by_edge)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\osmnx\\graph.py:577\u001B[0m, in \u001B[0;36m_create_graph\u001B[1;34m(response_jsons, retain_all, bidirectional)\u001B[0m\n\u001B[0;32m    574\u001B[0m     G\u001B[38;5;241m.\u001B[39madd_node(node, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;66;03m# add each osm way (ie, a path of edges) to the graph\u001B[39;00m\n\u001B[1;32m--> 577\u001B[0m \u001B[43m_add_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbidirectional\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    579\u001B[0m \u001B[38;5;66;03m# retain only the largest connected component if retain_all is False\u001B[39;00m\n\u001B[0;32m    580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m retain_all:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\osmnx\\graph.py:780\u001B[0m, in \u001B[0;36m_add_paths\u001B[1;34m(G, paths, bidirectional)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;66;03m# add all the edge tuples and give them the path's tag:value attrs\u001B[39;00m\n\u001B[0;32m    779\u001B[0m path[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreversed\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 780\u001B[0m G\u001B[38;5;241m.\u001B[39madd_edges_from(edges, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpath)\n\u001B[0;32m    782\u001B[0m \u001B[38;5;66;03m# if the path is NOT one-way, reverse direction of each edge and add\u001B[39;00m\n\u001B[0;32m    783\u001B[0m \u001B[38;5;66;03m# this path going the opposite direction too\u001B[39;00m\n\u001B[0;32m    784\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_one_way:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\networkx\\classes\\multigraph.py:585\u001B[0m, in \u001B[0;36mMultiGraph.add_edges_from\u001B[1;34m(self, ebunch_to_add, **attr)\u001B[0m\n\u001B[0;32m    583\u001B[0m         key \u001B[38;5;241m=\u001B[39m dd  \u001B[38;5;66;03m# ne == 3 with 3rd value not dict, must be a key\u001B[39;00m\n\u001B[0;32m    584\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_edge(u, v, key)\n\u001B[1;32m--> 585\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mu\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mv\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mddd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    586\u001B[0m     keylist\u001B[38;5;241m.\u001B[39mappend(key)\n\u001B[0;32m    587\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m keylist\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Download data from OSM\n",
    "if place =='Tel Aviv':\n",
    "    useful_tags_path = ['name:en','highway','length','bearing','tunnel','junction']\n",
    "    ox.utils.config(useful_tags_way=useful_tags_path)\n",
    "graph = ox.graph_from_place(place, network_type='all')\n",
    "graph = ox.bearing.add_edge_bearings(graph, precision=1)\n",
    "graph_pro = ox.projection.project_graph(graph, to_crs=project_crs)\n",
    "io.save_graph_geopackage(graph_pro, filepath=f'{data_folder}/osm_data.gpkg', encoding='utf-8', directed=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## Run when initialise the code\n",
    "# find and store roundabout\n",
    "my_gdf = gpd.read_file(f'{data_folder}/osm_data.gpkg',layer = 'edges')# Identify roundabout elements, if any exist, and store them in a separate DataFrame.\n",
    "if place =='Tel Aviv':\n",
    "    my_gdf.rename(columns={'name:en':'name'}, inplace=True)\n",
    "is_junction= True if 'junction' in my_gdf.columns else False\n",
    "if is_junction:\n",
    "    round_about = my_gdf[my_gdf['junction'].isin(['roundabout', 'circular'])]\n",
    "    my_gdf= my_gdf[~((my_gdf['junction'] == 'roundabout') | (my_gdf['junction'] == 'circular'))]\n",
    "\n",
    "# Remove additional irrelevant line objects based on values of the OSM 'tunnel' and 'highway' keys.\n",
    "# if 'tunnel' in my_gdf.columns:\n",
    "#     my_gdf = my_gdf[~((my_gdf['tunnel'] == 'building_passage') | (my_gdf['tunnel'] == 'yes'))]\n",
    "to_remove = my_gdf[~((my_gdf['highway'] == 'motorway') | (my_gdf['highway'] == 'trunk')| (my_gdf['highway'] == 'motorway_link')| (my_gdf['highway'] == 'motorway_link')| (my_gdf['highway'] == 'trunk_link'))]\n",
    "\n",
    "\n",
    "# Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.\n",
    "df_pro = to_remove.to_crs(project_crs).dropna(subset=['name'])\n",
    "df_pro = df_pro[df_pro['name']!='']\n",
    "df_pro['angle'] = df_pro['bearing'].apply(lambda x: x if x < 180 else x - 180)\n",
    "df_pro['length'] = df_pro.length\n",
    "\n",
    "# Function to convert valid list strings to lists\n",
    "def convert_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)[0]\n",
    "    except (ValueError, SyntaxError,TypeError):\n",
    "        return s  # Return the original string if conversion fails\n",
    "\n",
    "# Apply the function to the DataFrame column so polylines with several street names will return the first name and highway type.\n",
    "df_pro['name'] = df_pro['name'].apply(convert_to_list)\n",
    "df_pro['highway'] = df_pro['highway'].apply(convert_to_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# region\n",
    "\n",
    "# Functions and classes to be utilized - Module 2\n",
    "def check_parallelism(to_translate: GeoDataFrame) -> bool:\n",
    "    # See if there are parallel lines\n",
    "    my_buffer = to_translate['geometry'].buffer(cap_style=2, distance=30, join_style=3)\n",
    "    to_translate['geometry_right'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'right'))\n",
    "    to_translate['geometry_left'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'left')) # we need to offset by both sides since the parallel lines could be in opposite directions\n",
    "    def is_parallel(my_s_join: GeoDataFrame, the_buffer: GeoSeries, geo_field: str):\n",
    "        my_s_join['geometry'] = my_s_join[geo_field]\n",
    "        new_data_0 = my_s_join.sjoin(GeoDataFrame(geometry=the_buffer, crs=project_crs), how='inner').reset_index()\n",
    "        if not len(new_data_0):\n",
    "            return False\n",
    "        new_data_1= new_data_0[new_data_0['index'] != new_data_0['index_right']] # Remove overlay of polylines with its buffer\n",
    "        for translated_line in new_data_1.iterrows():\n",
    "            translated_line = translated_line[1]\n",
    "            geo_tr_line =GeoDataFrame(data= pd.DataFrame([translated_line]),crs=project_crs)\n",
    "            overlay = gpd.overlay(geo_tr_line, GeoDataFrame(geometry=the_buffer.loc[geo_tr_line['index_right']], crs=project_crs), how='intersection')\n",
    "            if (overlay.length/translated_line.length).iloc[0]*100>10:\n",
    "                return True\n",
    "        return False\n",
    "    if is_parallel(to_translate, my_buffer, 'geometry_right'):\n",
    "        return True\n",
    "    else:\n",
    "        if is_parallel(to_translate, my_buffer, 'geometry_left'):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "def create_center_line(one_poly):\n",
    "    \"\"\"\n",
    "    This method calculate new line between the farthest points of the simplified polygon\n",
    "    :param one_poly:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    lines_in_buffer = data.sjoin(GeoDataFrame(geometry=[one_poly],crs=project_crs)).drop(columns='index_right')\n",
    "\n",
    "    list_pnts_of_line_group= []\n",
    "    def update_list(line_local):\n",
    "        \"\"\"\n",
    "        add the first start/end point into the list\n",
    "        :param line_local:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        list_pnts_of_line_group.extend([Point(line_local.coords[0]), Point(line_local.coords[-1])])\n",
    "\n",
    "    # Get the start/end points of these polylines\n",
    "    lines_in_buffer['geometry'].apply(update_list)\n",
    "\n",
    "    # Find all the unidirectional combinations between each two pair of points\n",
    "    point_combinations = list(combinations(list_pnts_of_line_group, 2))\n",
    "\n",
    "\n",
    "    # Save it into DataFrame frame and calculate distance\n",
    "    df_test= DataFrame()\n",
    "    df_test['point_1'] = [pair[0] for pair in point_combinations]\n",
    "    df_test['point_2'] = [pair[1] for pair in point_combinations]\n",
    "    df_test['dist'] = df_test.apply(lambda x:x['point_1'].distance(x['point_2']),axis=1)\n",
    "\n",
    "\n",
    "    # Calculate  angle (0 and 180)\n",
    "    # Calculate angle using vectorized operations\n",
    "    # Vectorized angle calculation using NumPy\n",
    "    dx = df_test['point_2'].apply(lambda p: p.x)  - df_test['point_1'].apply(lambda p: p.x)\n",
    "    dy = df_test['point_2'].apply(lambda p: p.y)  - df_test['point_1'].apply(lambda p: p.y)\n",
    "    df_test['angle'] = np.degrees(np.arctan2(dy, dx))\n",
    "    df_test['angle'] = np.where(df_test['angle'] > 0, df_test['angle'], df_test['angle'] + 180)\n",
    "\n",
    "\n",
    "    # Calculate the best two points by looking on their distance and angle. we compare the angle to the polylines angles. The angle has less important so the reason for 0.5\n",
    "    avg= lines_in_buffer['angle'].mean()\n",
    "    dis = abs(df_test['angle'] -avg)\n",
    "    df_test['ratio'] = df_test['dist']/df_test['dist'].max() + 0.5*dis /dis.max()\n",
    "    max_points = df_test.sort_values(by='ratio',ascending=False).iloc[0]\n",
    "\n",
    "    # These points will be served to be initial reference in order to find more points\n",
    "    pnt_f = max_points['point_1']\n",
    "    pnt_l = max_points['point_2']\n",
    "\n",
    "    angl_rng = lines_in_buffer['angle'].max() - lines_in_buffer['angle'].min()\n",
    "    if angl_rng <1: # If the angel range is less than 1 degree the line will be based on the first and last points\n",
    "            lines_pnt_geo = [pnt_f]\n",
    "    else:\n",
    "        if angl_rng > 100: # Maximum of length to check is every 10 meters\n",
    "            length_to_check  =10\n",
    "        else:\n",
    "            length_to_check  =75-log2(angl_rng)*10 # The range of  length_to_check (logarithm to create more changes at the beginning)\n",
    "        lines_pnt_geo = add_more_pnts_to_new_lines(pnt_f, pnt_l, [pnt_f],length_to_check,lines_in_buffer)\n",
    "    lines_pnt_geo.append(pnt_l)\n",
    "    # Update dic_final\n",
    "    return lines_pnt_geo\n",
    "def add_more_pnts_to_new_lines(pnt_f_loc: Point, pnt_l_loc: Point, line_pnts: list, lngth_chck: float, test_poly:GeoDataFrame) -> list:\n",
    "    \"\"\"\n",
    "    This method checks if more points should be added to the new lines by checking along the new line if the distance to the old network roads are more than 10 meters\n",
    "    :param test_poly: From these polylines find the closet one in each interation\n",
    "    :param lngth_chck: Used latter to find how many checks should be done\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Calculate distance and azimuth between the first and last point\n",
    "    dist = pnt_f_loc.distance(pnt_l_loc)\n",
    "    x_0 = pnt_f_loc.coords[0][0]\n",
    "    y_0 = pnt_f_loc.coords[0][1]\n",
    "    bearing = math.atan2(pnt_l_loc.coords[0][0] - x_0, pnt_l_loc.coords[0][1] - y_0)\n",
    "    bearing = bearing + 2 * math.pi if bearing < 0 else bearing\n",
    "    # Calculate the number of  checks going to carry out\n",
    "    loops = int(dist / lngth_chck)\n",
    "    # Calculate  the first point over the line\n",
    "    for dis_on_line in range(1, loops):\n",
    "        x_new = x_0 + lngth_chck * dis_on_line * math.sin(bearing)\n",
    "        y_new = y_0 + lngth_chck * dis_on_line * math.cos(bearing)\n",
    "        # S_joins to all the network lines (same name and group)\n",
    "        # if the distance is less than 10 meters continue, else: find the projection point and add it to the correct location and run the function agein\n",
    "        one_pnt_df = GeoDataFrame(geometry=[Point(x_new, y_new)], crs=project_crs)\n",
    "        s_join_loc = one_pnt_df.sjoin_nearest(test_poly, distance_col='dis').iloc[0]\n",
    "\n",
    "        if s_join_loc['dis'] > 10:\n",
    "            line = data.loc[s_join_loc['index_right']]['geometry']\n",
    "            pnt_med= line.interpolate(line.project( s_join_loc['geometry']))\n",
    "            if pnt_med.distance(pnt_f_loc)<10: # Otherwise the code may stack in endless loops\n",
    "                continue\n",
    "            line_pnts.append(pnt_med)\n",
    "            line_pnts = add_more_pnts_to_new_lines(pnt_med, pnt_l_loc, line_pnts,lngth_chck,test_poly)\n",
    "            return line_pnts\n",
    "    return line_pnts\n",
    "def update_df_with_center_line(new_line,is_simplified=0,group_name= -1):\n",
    "    \"\"\"\n",
    "    update our dictionary with new lines\n",
    "    :param is_simplified:\n",
    "    :param new_line:\n",
    "    :param group_name: According to the DBSCAN algorithm, if no =-1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dic_final['name'].append(name)\n",
    "    # dic_final['geometry'].append(LineString(coordinates=(pnt_list[max_dis[0]], pnt_list[max_dis[1]])))\n",
    "    dic_final['geometry'].append(new_line)\n",
    "    dic_final['highway'].append(data.iloc[0]['highway'])\n",
    "    dic_final['bearing'].append(data['angle'].mean())\n",
    "    dic_final['group'].append(group_name)\n",
    "    dic_final['is_simplified'].append(is_simplified)\n",
    "\n",
    "# Initiate dic_final here for @def update_df_with_center_line\n",
    "dic_final = {'name': [], 'geometry': [], 'highway': [], 'bearing': [], 'group': [],'is_simplified':[]}\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dic_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Make sure @dic_final is empty before execute this code\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m dic_final\u001B[38;5;241m=\u001B[39m {key: [] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdic_final\u001B[49m}\n\u001B[0;32m      3\u001B[0m my_groupby \u001B[38;5;241m=\u001B[39m df_pro\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# group the street segments by street name\u001B[39;00m\n\u001B[0;32m      4\u001B[0m for_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(my_groupby)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dic_final' is not defined"
     ]
    }
   ],
   "source": [
    "# Make sure @dic_final is empty before execute this code\n",
    "dic_final= {key: [] for key in dic_final}\n",
    "my_groupby = df_pro.groupby('name') # group the street segments by street name\n",
    "for_time = len(my_groupby)\n",
    "\n",
    "number_of_parallel = 0 # count the number of polylines were refined\n",
    "with tqdm(total=for_time) as pbar: #  It is used in order to visualise the progress by progress bar\n",
    "    for i, street in enumerate(my_groupby):\n",
    "        res = street[1] # it holds all the streets\n",
    "        name = street[0] # It holds the streets name\n",
    "        pbar.update(1) # for the progress bar\n",
    "        # Remove segments without angle. If less than two segments being left move to the next group.\n",
    "        res = res.dropna(subset=['angle'], axis=0)\n",
    "        if len(res) < 2:\n",
    "            data = res\n",
    "            _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "            continue\n",
    "        # Use DBSCAN to classify streets based on their angle, and group each class. Outliers could not consider parallel with any street, thus removed\n",
    "        res['group'] = DBSCAN(eps=10, min_samples=2).fit(res['angle'].to_numpy().reshape(-1, 1)).labels_\n",
    "        # if all is -1, don't touch the element\n",
    "        if (res['group']== -1).all():\n",
    "            data = res\n",
    "            _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "            continue\n",
    "        # cur_group = res[(res['group'] > -1) | (res.length>20)].groupby('group') # Remove short segments with -1 classification values\n",
    "        # The parallel test is on street segments that  have the same name and belong to the same angle group.\n",
    "        for group in res.groupby('group'):\n",
    "            data = group[1]\n",
    "            if group[0] ==-1: # No need to check if is parallel\n",
    "                _  = data['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "                continue\n",
    "            if check_parallelism(data.copy()):\n",
    "                # print(group[0])\n",
    "                # Remove unimportant streets which appear more than 10% in the group\n",
    "                min_num_of_polylines = len(data)/10\n",
    "                # Use a single boolean condition for filtering\n",
    "                condition = (data['highway'].isin(['service','unclassified'])) & (data.groupby('highway')['highway'].transform('count') <= min_num_of_polylines)\n",
    "                data = data[~condition]\n",
    "\n",
    "                number_of_parallel+=len(data) # Update the number of parallel polylines\n",
    "\n",
    "                # unify lines to one polygon\n",
    "                buffers = data.buffer(cap_style=3, distance=30, join_style=3)\n",
    "                one_buffer = buffers.unary_union\n",
    "                # simplify polygon with simplify function. If one_buffer is multipolygon object simplify each one them separately\n",
    "                if isinstance(one_buffer, MultiPolygon):\n",
    "                    for polygon in one_buffer:\n",
    "                        lines_pnt_geo_final = create_center_line(polygon)\n",
    "                        update_df_with_center_line(LineString(lines_pnt_geo_final),1,group[0])\n",
    "                else:\n",
    "                    lines_pnt_geo_final =create_center_line(one_buffer)\n",
    "                    # Update dic_final\n",
    "                    update_df_with_center_line(LineString(lines_pnt_geo_final),1,group[0])\n",
    "\n",
    "            else:\n",
    "                _  = data['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "\n",
    "print(number_of_parallel)\n",
    "print('create new files')\n",
    "# remove short lines\n",
    "final_cols = ['name', 'geometry', 'highway', 'bearing', 'length']\n",
    "new_network = GeoDataFrame(dic_final, crs=project_crs)\n",
    "new_network['length']= new_network.length\n",
    "# create network\n",
    "new_network.to_file(f'{smplfcton_fldr}/simp.shp')\n",
    "# save the data as a pickle format\n",
    "\n",
    "# save another file for test\n",
    "get_five_largest_streets(new_network,'name',f'{smplfcton_fldr}/test_simp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "# region\n",
    "# Classes to be employed during the execution of this code.\n",
    "#Intersection\n",
    "#Split in intersection\n",
    "class Intersection:\n",
    "    def __init__(self,network:GeoDataFrame,number:int):\n",
    "        \"\"\"\n",
    "\n",
    "        :param network:\n",
    "        :param number: give a unique name to the files created during the process (this class will be use again in this code)\n",
    "        \"\"\"\n",
    "        self.my_network = network\n",
    "        self.inter_pnt_dic = {'geometry':[],'name':[]}\n",
    "        self.lines_to_delete =[]\n",
    "        self.num = number\n",
    "\n",
    "\n",
    "    def intersection_network(self):\n",
    "        \"\"\"\n",
    "        This function fix topology (add or remove vertices) where needed\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # First remove_false_nodes\n",
    "        self.my_network = remove_false_nodes(self.my_network).reset_index(drop=True)\n",
    "        self.my_network.to_file(f'{data_folder}/delete_2_nodes/delete_false_intersection.shp')\n",
    "        # Create buffer around each element\n",
    "        buffer_around_lines= self.my_network['geometry'].buffer(cap_style=3, distance=1, join_style=3)\n",
    "\n",
    "\n",
    "        # s_join between buffer to lines\n",
    "        s_join_0 =gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_lines,crs=project_crs),right_df=self.my_network)\n",
    "\n",
    "        # delete lines belong to the buffer\n",
    "        s_join = s_join_0[s_join_0.index!=s_join_0['index_right']]\n",
    "\n",
    "\n",
    "        # Find new intersections that are not at the beginning or end of the line\n",
    "        for_time =len(s_join)\n",
    "        with tqdm(total=for_time) as pbar:\n",
    "            s_join.apply(lambda x: self.find_intersection_points(x,pbar), axis=1)\n",
    "        if len(self.inter_pnt_dic)==0:\n",
    "            return\n",
    "        inter_pnt_gdf = GeoDataFrame(self.inter_pnt_dic,crs=project_crs)\n",
    "        inter_pnt_gdf.to_file(f'{data_folder}/delete_2_nodes/inter_pnt_dic.shp')\n",
    "        # Split string line by points\n",
    "        segments = {'geometry':[],'org_id':[]}\n",
    "        # Groupby points name (which is the line they should split)\n",
    "        my_groups =  inter_pnt_gdf.groupby('name')\n",
    "        for_time = len(my_groups)\n",
    "        with  tqdm(total=for_time) as pbar:\n",
    "            for group_pnts in my_groups:\n",
    "                pbar.update(1)\n",
    "                points  = group_pnts[1]\n",
    "                points['is_split'] = True\n",
    "                # if group_pnts[0]==588:\n",
    "                #     print(points)\n",
    "                # get the line to split by comparing the name\n",
    "                row = self.my_network.loc[group_pnts[0]]\n",
    "                current = list(row.geometry.coords)\n",
    "                points_line = [Point(x) for x in current]\n",
    "                points_line_gdf = GeoDataFrame(geometry=points_line,crs=project_crs)\n",
    "                points_line_gdf['is_split'] = False\n",
    "\n",
    "                # append all the points together (line points and split points)\n",
    "                line_all_pnts = points_line_gdf.append(points)\n",
    "\n",
    "                # Find the distance of each point form the begining of the line on the line.\n",
    "                line_all_pnts['dis_from_the_start'] = line_all_pnts['geometry'].apply(lambda x:row.geometry.project(x))\n",
    "                line_all_pnts.sort_values('dis_from_the_start',inplace=True)\n",
    "\n",
    "                # split the line\n",
    "                seg =[]\n",
    "                for point in line_all_pnts.iterrows():\n",
    "                    prop = point[1]\n",
    "                    seg.append(prop['geometry'])\n",
    "                    if prop['is_split']:\n",
    "                        segments['geometry'].append(LineString(seg))\n",
    "                        segments['org_id'].append(row.name)\n",
    "                        seg = [prop['geometry']]\n",
    "                # if the split point is the last one, you don't need to create new segment\n",
    "                if len(seg)>1:\n",
    "                    segments['geometry'].append(LineString(seg))\n",
    "                    segments['org_id'].append(row.name)\n",
    "        network_split = GeoDataFrame(data=segments,crs=project_crs)\n",
    "        network_split.to_file(f'{data_folder}/delete_2_nodes/segments.shp')\n",
    "        cols_no_geometry = self.my_network.columns[:-1]\n",
    "        network_split_final = network_split.set_index('org_id')\n",
    "        network_split_final[cols_no_geometry] =self.my_network[cols_no_geometry]\n",
    "        # remove old and redundant line from our network and update with new one\n",
    "        network_split =self.my_network.drop(index=network_split_final.index.unique()).append(network_split_final)\n",
    "        network_split['length'] = network_split.length\n",
    "        self.my_network = network_split\n",
    "        self.my_network.reset_index(drop=True,inplace= True)\n",
    "        self.my_network.to_file(f'{data_folder}/delete_2_nodes/intersection_network.shp')\n",
    "\n",
    "    def find_intersection_points(self,row,pbar):\n",
    "        r\"\"\"\n",
    "        find the intersection points between the two lines\n",
    "        :param row:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pbar.update(1)\n",
    "            line_1 = self.my_network.loc[row.name]\n",
    "            line_2 =  self.my_network.loc[row['index_right']]\n",
    "            pnt = line_1.geometry.intersection(line_2.geometry)\n",
    "            # If there are more than one intersection between two lines, one of the lines should be deleted.\n",
    "            if isinstance(pnt,LineString): # The intersection is only between the buffer and the point ToDo - when the situation is a overlay line\n",
    "                return\n",
    "            if isinstance(pnt,MultiPoint):\n",
    "                for single_pnt in pnt:\n",
    "                    self.inter_pnt_dic['geometry'].append(single_pnt)\n",
    "                    self.inter_pnt_dic['name'].append(row.name)\n",
    "                return\n",
    "            # If it is first or end continue OR if there is no intersection between the two lines\n",
    "            if len(pnt.coords)==0 or pnt.coords[0]==line_1.geometry.coords[0] or pnt.coords[0]==line_1.geometry.coords[-1]:\n",
    "                return\n",
    "            self.inter_pnt_dic['geometry'].append(pnt)\n",
    "            self.inter_pnt_dic['name'].append(row.name)\n",
    "        except:\n",
    "            print(f\"{row.name},{row['index_right']}:{pnt}\")\n",
    "\n",
    "    def update_names(self, org_gpd:GeoDataFrame):\n",
    "        \"\"\"\n",
    "        It updates the name of those lost their name during the previous process\n",
    "        :param org_gpd:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        df1 = self.my_network\n",
    "        # Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "        df3 = df1[df1['name'].notna()]\n",
    "        # df3.to_file(f'{data_folder}/delete_2_nodes/with_name.shp')\n",
    "        df4 = df1[df1['name'].isna()]\n",
    "        # df4.reset_index().to_file(f'{data_folder}/delete_2_nodes/no_name_init.shp')\n",
    "\n",
    "        # use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "        old_index  ='old_index'\n",
    "        df4_as_buffer= GeoDataFrame(geometry=df4['geometry'].buffer(distance  = 2, cap_style=2),crs=project_crs)\n",
    "        df = gpd.sjoin(df4_as_buffer, org_gpd) # for spatial join use buffer around each polyline.that provide better result\n",
    "        df.index.name = old_index\n",
    "        df['geometry'] = df4['geometry'] # bring the dataframe into linestring format\n",
    "        df.reset_index(inplace=True) # To be consistent with the following code and other dataframe\n",
    "        # Create a new dictionary to store the updated data.\n",
    "        dic_str_data = []\n",
    "\n",
    "\n",
    "        def return_street_name(aplcnts_tst):\n",
    "            \"\"\"\n",
    "            1. \"Count the occurrences of polylines with the same name within each aplcnts_tst.\"\n",
    "            2. \"Return the street if a aplcnts_tst contains only one unique street name.\"\n",
    "            3. \"If a single street name predominates within a aplcnts_tst, return that name.\"\n",
    "            4. \"For groups with multiple names, perform a buffer calculation around the respective polylines and determine the largest overlapping area, returning the name associated with that area.\"\n",
    "            :param aplcnts_tst: group of applicants. Some of them hold the correct street name\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            count_names = aplcnts_tst['name'].value_counts().sort_values(ascending=False)\n",
    "            if len(count_names)==1:\n",
    "                # there is only one name\n",
    "                my_data = aplcnts_tst.iloc[0]\n",
    "            elif count_names[1]- count_names[0]>1:\n",
    "                # The highest number of polylines with the same name are bigger at least in 2:\n",
    "                my_data = aplcnts_tst[aplcnts_tst['name'] == count_names.index[0]].iloc[0]\n",
    "            else:\n",
    "                # otherwise filter those with the most popular name or close to (-1)\n",
    "                str_to_wrk_on  =aplcnts_tst[aplcnts_tst['name'].isin(count_names[count_names - count_names[0] < 2].index)]\n",
    "                buffer_0 = GeoDataFrame(geometry=[str_to_wrk_on.iloc[0]['geometry'].buffer(distance  = 20, cap_style=2)],crs=project_crs) # Buffer around the polyline without name\n",
    "\n",
    "                streets_right_geo = org_gpd[org_gpd.index.isin(str_to_wrk_on['index_right'])].reset_index() # Get all the applicants polylines and create buffer around\n",
    "                buffer_1 =GeoDataFrame(geometry=streets_right_geo.buffer(distance  = 20, cap_style=2))\n",
    "                streets_right_geo['area'] =gpd.overlay(buffer_1, buffer_0, how='intersection').area\n",
    "                groupy = streets_right_geo.groupby('name')\n",
    "                my_data_0 = groupy.get_group(groupy.sum()['area'].sort_values(ascending=False).index[0]).sort_values(by= 'area',ascending=False).iloc[0]\n",
    "                # Get back to the @aplcnts_tst and find the relevant row by comparing index\n",
    "                my_data = aplcnts_tst[aplcnts_tst['index_right'] == my_data_0['index']].iloc[0]\n",
    "            # Populate the new dictionary with relevant data\n",
    "            dic_str_data.append(my_data.to_list())\n",
    "        _ =df.groupby(old_index).apply(return_street_name)\n",
    "        # convert the dictionary into a dataframe.\n",
    "        updated_df = GeoDataFrame(data= dic_str_data,columns=df.columns,crs=project_crs).drop(columns='index_right').set_index(old_index)\n",
    "        updated_df['length'] = updated_df.length\n",
    "        self.my_network = df3.append(updated_df)\n",
    "\n",
    "#Roundabout\n",
    "class EnvEntity:\n",
    "        def __init__(self,network):\n",
    "            self.dead_end_fd = None\n",
    "            self.pnt_dead_end = None\n",
    "            self.pnt_dic = {}\n",
    "            self.first_last_dic = {'geometry': [], 'line_name': [], 'position': []}\n",
    "            self.network = network\n",
    "\n",
    "\n",
    "        def __populate_pnt_dic(self,point: type, name_of_line: str):\n",
    "            \"\"\"\n",
    "            Make \"pnt_dic\" contain a list of all the lines connected to each point.\n",
    "            :param point:\n",
    "            :param name_of_line:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            if not point in self.pnt_dic:\n",
    "                self.pnt_dic[point] = []\n",
    "            self.pnt_dic[point].append(name_of_line)\n",
    "\n",
    "        def __send_pnts(self,temp_line: GeoSeries):\n",
    "            \"\"\"\n",
    "            # Send the first and the last points to populate_pnt_dic\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            my_geom = temp_line['geometry']\n",
    "            self.__populate_pnt_dic(my_geom.coords[0], temp_line.name)\n",
    "            self.__populate_pnt_dic(my_geom.coords[-1], temp_line.name)\n",
    "\n",
    "        def get_deadend_gdf(self,delete_short:int =30)-> GeoDataFrame:\n",
    "            self.network.apply(self.__send_pnts, axis=1)\n",
    "\n",
    "            deadend_list = [item[1][0] for item in self.pnt_dic.items() if len(item[1]) == 1]\n",
    "            pnt_dead_end_0 = [item for item in self.pnt_dic.items() if len(item[1]) == 1] # Retain all the line points with deadened\n",
    "            self.pnt_dead_end = [Point(x[0]) for x in pnt_dead_end_0]\n",
    "            # Create shp file of deadened_pnts\n",
    "            geometry,line_name = 'geometry','line_name'\n",
    "            pnt_dead_end_df = GeoDataFrame(data=pnt_dead_end_0)\n",
    "            pnt_dead_end_df[geometry]= pnt_dead_end_df[0].apply(lambda x:Point(x))\n",
    "            pnt_dead_end_df[line_name] = pnt_dead_end_df[1].apply(lambda x:x[0])\n",
    "            pnt_dead_end_df.crs = project_crs\n",
    "            self.dead_end_fd = pnt_dead_end_df\n",
    "\n",
    "            if delete_short>0:\n",
    "                # If it is necessary to eliminate dead-end short segments, it is  important to delete them from the network geodataframe.\n",
    "\n",
    "                deadend_gdf =self.network.loc[deadend_list]\n",
    "                self.network.drop(index=deadend_gdf[deadend_gdf.length<delete_short].index,inplace=True)\n",
    "                return deadend_gdf[deadend_gdf.length>delete_short]\n",
    "            return self.network.loc[deadend_list]\n",
    "\n",
    "        def update_the_current_network(self,temp_network):\n",
    "            r\"\"\"\n",
    "            Update the current network in the new changes\n",
    "            :param temp_network:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            new_network_temp = self.network.drop(index=temp_network.index)\n",
    "            self.network = new_network_temp.append(temp_network)\n",
    "            self.network['length'] = self.network.length\n",
    "            self.network  = self.network[self.network['length']>1]\n",
    "class Roundabout(EnvEntity):\n",
    "    def __init__(self,network: GeoDataFrame):\n",
    "       EnvEntity.__init__(self,network)\n",
    "       self.pnt_dic ={}\n",
    "       self.centroid =self.__from_roundabout_to_centroid()\n",
    "       self.network.rename(columns={'name': 'str_name'}, inplace=True)\n",
    "    def __from_roundabout_to_centroid(self):\n",
    "        # Find the center of each roundabout\n",
    "        # create polygon around each polygon and union\n",
    "        round_about_buffer = round_about.to_crs(project_crs)['geometry'].buffer(cap_style=1, distance=10,\n",
    "                                                                                join_style=1).unary_union\n",
    "        dic_data = {'name': [], 'geometry': []}\n",
    "        if round_about_buffer.type=='Polygon': # In case we have only one polygon\n",
    "            dic_data['name'].append(0)\n",
    "            dic_data['geometry'].append(round_about_buffer.centroid)\n",
    "        else:\n",
    "            for ii, xx in enumerate(round_about_buffer):\n",
    "                dic_data['name'].append(ii)\n",
    "                dic_data['geometry'].append(xx.centroid)\n",
    "        centroid =GeoDataFrame(dic_data, crs=project_crs)\n",
    "        return centroid\n",
    "        # GeoDataFrame(dic_data,crs=project_crs).to_file(f'{path_round_about}/roundabout_union.shp')\n",
    "\n",
    "    def __first_last_pnt_of_line(self,row: GeoSeries):\n",
    "        r\"\"\"\n",
    "        It get geometry of line and fill the first_last_dic with the first and last point and the name of the line\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        geo = list(row['geometry'].coords)\n",
    "        self.first_last_dic['geometry'].extend([Point(geo[0]), Point(geo[-1])])\n",
    "        self.first_last_dic['line_name'].extend([row.name] * 2)\n",
    "        self.first_last_dic['position'].extend([0, -1])\n",
    "    def deadend(self):\n",
    "        r\"\"\"\n",
    "        remove not connected line shorter than 100 meters and then return deadend_list lines and their endpoints (as another file)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Find the first and last points\n",
    "\n",
    "        # Get deadend_gdf\n",
    "        deadend_gdf = self.get_deadend_gdf()\n",
    "\n",
    "        # Create gdf of line points with the reference to the line they belong\n",
    "        deadend_gdf.apply(self.__first_last_pnt_of_line, axis=1)\n",
    "        first_last_gdf = GeoDataFrame(self.first_last_dic, crs=project_crs)\n",
    "\n",
    "\n",
    "        return deadend_gdf, first_last_gdf\n",
    "    def __update_geometry(self,cur,s_join):\n",
    "        r\"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if cur['highway'] == 'footway':\n",
    "            # Don't snap footway to roundabout\n",
    "            return cur['geometry']\n",
    "        # Get only the points that are deadened\n",
    "        points_lines = [item for item in s_join[s_join['line_name'] == cur.name].iterrows()if item[1]['geometry'] in self.pnt_dead_end]\n",
    "        if len(points_lines) == 0:\n",
    "            # No roundabout nearby\n",
    "            return cur['geometry']\n",
    "        # get the line geometry to change the first and/ or last point\n",
    "        geo_cur = list(cur['geometry'].coords)\n",
    "\n",
    "        # iterate over the deadened points  near roundabout\n",
    "        for ind in range(len(points_lines)):\n",
    "            points_line = points_lines[ind]\n",
    "            geo_cur[points_line[1]['position']] = self.centroid.loc[points_line[1]['index_right']]['geometry'].coords[\n",
    "                0]\n",
    "        return LineString(geo_cur)\n",
    "    def my_spatial_join(self,deadend_lines, deadend_pnts,line_name):\n",
    "        # Spatial join between roundabout centroid to nearby dead end lines\n",
    "        # centroid = gpd.read_file(f'{path_round_about}/centroid.shp')\n",
    "        s_join = gpd.sjoin_nearest(left_df=deadend_pnts, right_df=self.centroid, how='left', max_distance=100,\n",
    "                                   distance_col='dist').dropna(subset='dist')\n",
    "\n",
    "        # Deadened lines from both lines should be removed\n",
    "        lines_to_delete_test = s_join['line_name'].unique() # all the Deadened lines close to roundabout\n",
    "\n",
    "        # All deadened lines from both lines\n",
    "        deads_both_side = self.dead_end_fd['line_name'].value_counts()\n",
    "        deads_both_side =deads_both_side[deads_both_side==2]\n",
    "\n",
    "        # Remove this lines from the database\n",
    "        lines_to_delete=deads_both_side[deads_both_side.index.isin(lines_to_delete_test)]\n",
    "\n",
    "        self.network = self.network[~((self.network[line_name].isin(lines_to_delete.index)) & (self.network.length<300))]\n",
    "        deadend_lines = deadend_lines[~((deadend_lines[line_name].isin(lines_to_delete.index)) & (deadend_lines.length<300))]\n",
    "        # Update the geometry so the roundabout will be part of the line geometry\n",
    "        change_geo = deadend_lines.copy()\n",
    "\n",
    "        change_geo['geometry'] = change_geo.apply(lambda x:self.__update_geometry(x,s_join), axis=1)\n",
    "\n",
    "        return change_geo\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# starting point\n",
    "new_network = gpd.read_file(f'{smplfcton_fldr}/simp.shp').rename(columns={'is_simplif':'is_simplified'})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 37316/37316 [00:06<00:00, 6098.03it/s]\n",
      "100%|| 1915/1915 [00:10<00:00, 179.37it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num=0\n",
    "new_gpd = new_network.copy()\n",
    "obj_intersection = Intersection(new_gpd,num)\n",
    "obj_intersection.intersection_network()\n",
    "obj_intersection.update_names(new_gpd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# For test\n",
    "obj_intersection.my_network.reset_index().to_file(f'{data_folder}/intersection/all_new2.shp')\n",
    "get_five_largest_streets(obj_intersection.my_network,'name',f'{data_folder}/intersection/test_all_new2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Start point is parameter, consider to update\n",
    "start_point = True\n",
    "line_name ='line_name'\n",
    "if start_point:\n",
    "    exist_data = gpd.read_file(f'{data_folder}/intersection/all_new2.shp').reset_index(names=line_name)\n",
    "else:\n",
    "    exist_data= obj_intersection.my_network.reset_index().reset_index(names=line_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if is_junction:\n",
    "\n",
    "    exist_data= obj_intersection.my_network.reset_index().reset_index(names=line_name)\n",
    "    my_roundabout=Roundabout(exist_data)\n",
    "    deadend_lines, deadend_pnts = my_roundabout.deadend()\n",
    "\n",
    "    # update the current network\n",
    "    change_geo = my_roundabout.my_spatial_join(deadend_lines, deadend_pnts,line_name)\n",
    "    my_roundabout.update_the_current_network(change_geo)\n",
    "\n",
    "    my_roundabout.network.drop_duplicates(subset=line_name,inplace=True)\n",
    "    # Improve roundabout\n",
    "    # First buffer around centroid\n",
    "    centr_name= 'centr_name'\n",
    "    buffer_around_centroid= my_roundabout.centroid['geometry'].buffer(cap_style=1, distance=30)\n",
    "\n",
    "    # s_join between buffer to lines (reset index to retain the original centroid name which can apper more than one in the results). always stay with data you need and with understandable name\n",
    "    roundabout_with_lines =gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_centroid,crs=project_crs).reset_index(),right_df=my_roundabout.network[['geometry',line_name]]).drop_duplicates(subset=['index',line_name]).rename(columns={\"index\":centr_name})[['geometry',line_name,centr_name]]\n",
    "\n",
    "    # To facilitate the searching process\n",
    "    my_roundabout.network.set_index(line_name,inplace=True)\n",
    "    # To facilitate easy access to point centroid geometry data, it is advisable to store the information in an object that provides efficient retrieval.\n",
    "    pnt_centroid_temp = my_roundabout.centroid['geometry']\n",
    "    #  Group the data by centroid\n",
    "    for center_line in roundabout_with_lines.groupby(centr_name):\n",
    "        #  Iterate over each group after performing a groupby() operation\n",
    "        for center in center_line[1].itertuples():\n",
    "            # Find the line that connects to the current centroid and obtain its vertices\n",
    "            line_to_test = my_roundabout.network.loc[center[2]]\n",
    "            vertices_line = list(line_to_test['geometry'].coords)\n",
    "            pnt_test = [vertices_line[0],vertices_line[-1]]\n",
    "            # To determine if the current line is already connected to the current centroid,.\n",
    "            is_connected = my_roundabout.centroid[my_roundabout.centroid['geometry'].isin([Point(pnt_test[0]),Point(pnt_test[-1])])]\n",
    "            if len(is_connected)>0 and center[3] in is_connected['name']:\n",
    "                continue\n",
    "\n",
    "            if len(vertices_line)==2:\n",
    "                vertices_line.insert(1, pnt_centroid_temp[center[3]])\n",
    "            else:\n",
    "                my_list = [pnt_centroid_temp[center[3]].distance(Point(temp)) for temp in vertices_line]\n",
    "                # Find the minimum index\n",
    "                min_index = min(range(len(my_list)), key=my_list.__getitem__)\n",
    "                if min_index ==0:\n",
    "                    vertices_line.insert(0,pnt_centroid_temp[center[3]])\n",
    "                elif min_index == len(my_list)-1:\n",
    "                    vertices_line.append(pnt_centroid_temp[center[3]])\n",
    "                else:\n",
    "                    vertices_line[min_index] = pnt_centroid_temp[center[3]]\n",
    "            new_geo = LineString(vertices_line)\n",
    "            my_roundabout.network.at[center[2],'geometry'] = new_geo\n",
    "\n",
    "    new_network1 = my_roundabout.network.reset_index()\n",
    "    new_network1.drop(columns='index',inplace=True)\n",
    "\n",
    "    # Function to remove self-intersecting LineString geometries\n",
    "    def remove_self_intersecting(line):\n",
    "        return line.is_simple\n",
    "\n",
    "    # Apply the function to filter out self-intersecting geometries\n",
    "    new_network2= new_network1[new_network1['geometry'].apply(remove_self_intersecting)]\n",
    "    new_network_test= new_network1[~new_network1['geometry'].apply(remove_self_intersecting)]\n",
    "else:\n",
    "    new_network2=  obj_intersection.my_network.reset_index()\n",
    "\n",
    "new_network2.reset_index().to_file(f'{data_folder}/ra_network.shp')\n",
    "new_network_test.to_file(f'{data_folder}/new_network_test.shp')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "extend_lines_f= extend_lines(new_network2,100)\n",
    "extend_lines_f['length'] = extend_lines_f.length\n",
    "new_gpd = extend_lines_f.copy()\n",
    "# extend_lines_f.to_file(f'{data_folder}/extend_network.shp')\n",
    "\n",
    "obj_intersection_1 = Intersection(extend_lines_f.copy(),1)\n",
    "obj_intersection_1.intersection_network()\n",
    "obj_intersection_1.my_network.rename(columns={'str_name':'name'},inplace=True) # 'str_name' become 'str to be compatible with other previous networks\n",
    "new_gpd.rename(columns={'str_name':'name'},inplace=True)\n",
    "obj_intersection_1.update_names(org_gpd=new_gpd)\n",
    "\n",
    "# Clear short segments\n",
    "final2 = EnvEntity(obj_intersection_1.my_network.reset_index())\n",
    "final2.update_the_current_network(final2.get_deadend_gdf(delete_short=100))\n",
    "final2.network.drop(columns='bearing',inplace=True)\n",
    "final2.network.to_file(f'{data_folder}/final_test.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Aggregation\n",
    "intersection_agg_folder = f'{data_folder}/intersection_agg/'\n",
    "network = gpd.read_file(f'{data_folder}/final_test.shp')\n",
    "\n",
    "# 1. Get the first/start of each line\n",
    "# Extract unique start and end points from all LineStrings\n",
    "geometry= 'geometry'\n",
    "index_right= 'index_right'\n",
    "all_points = network[geometry].apply(lambda line: [Point(line.coords[0]), Point(line.coords[-1])]).explode()\n",
    "# # Create a GeoSeries of unique points\n",
    "unique_points = GeoDataFrame(geometry=gpd.GeoSeries(all_points).unique(),crs=project_crs)\n",
    "# save data\n",
    "unique_points.to_file(f'{intersection_agg_folder}unique_points.shp')\n",
    "# 2. Make sure I have the name of the lines associated with these lines\n",
    "pnts_line_name = unique_points.sjoin(network)[[index_right,geometry]].reset_index().dissolve(by='index',aggfunc=lambda x: x.tolist())\n",
    "pnts_line_name['num_of_lines']= pnts_line_name[index_right].apply(len) # count the number of lines for each point\n",
    "\n",
    "# 3. Use DBSCAN with 20 meters threshold\n",
    "# Extract coordinates for DBSCAN\n",
    "coordinates = pnts_line_name.geometry.apply(lambda point: (point.x, point.y)).tolist()\n",
    "dbscan = DBSCAN(eps=40, min_samples=2)\n",
    "pnts_line_name['group'] = dbscan.fit_predict(coordinates)\n",
    "lines_to_update = pnts_line_name[pnts_line_name['group']>-1]\n",
    "\n",
    "# if you want to save the files\n",
    "def save_points_file(data,path):\n",
    "    \"\"\"\n",
    "    The function get a data, arrange columns, convert list to string and export  it into a shpfile\n",
    "    :param data:\n",
    "    :param path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    col_of_lists_as_str= 'col_of_lists_as_str'\n",
    "    data[col_of_lists_as_str] = data[index_right].apply(str)\n",
    "    data.drop(columns= [index_right]).to_file(path)\n",
    "    data.drop(columns= [col_of_lists_as_str],inplace=True)\n",
    "save_points_file(pnts_line_name,f'{intersection_agg_folder}pnts_line_name.shp')\n",
    "save_points_file(lines_to_update,f'{intersection_agg_folder}lines_to_update.shp')\n",
    "\n",
    "# 4.1.Find the point with the max number of connected lines, if it is one use it otherwise uses the average\n",
    "# Find the maximum 'num' value for each group\n",
    "num = 'num_of_lines'\n",
    "group_name  = 'group'\n",
    "new_geometry= 'new_geometry'\n",
    "max_values_per_group =lines_to_update.groupby('group')['num_of_lines'].max()\n",
    "# Filter rows with the maximum 'num' value for each group\n",
    "result_gdf = lines_to_update[lines_to_update.set_index([group_name, num]).index.isin(max_values_per_group.items())]\n",
    "\n",
    "# Custom aggregation function to calculate the average point for each group\n",
    "def calculate_average_point(group):\n",
    "    x_mean = group.x.mean()\n",
    "    y_mean = group.y.mean()\n",
    "    return Point(x_mean, y_mean)\n",
    "\n",
    "# Apply the custom aggregation function to calculate average points per group\n",
    "lines_to_update2= lines_to_update.set_index(group_name)\n",
    "lines_to_update2['new_geometry'] = result_gdf.groupby(group_name )[geometry].apply(calculate_average_point)\n",
    "\n",
    "\n",
    "# 4.2 Among whom are updated remove every line the start and last point are the same\n",
    "# Get all the lines going to be deleted\n",
    "lines_to_delete =[]\n",
    "\n",
    "def update_lines_to_delete(row):\n",
    "    # explode the lines names within each row list to separate rows\n",
    "    lines_to_update_tmep = row[index_right].explode()\n",
    "\n",
    "    # Identify rows with duplicate values\n",
    "    lines_to_delete.extend(lines_to_update_tmep[lines_to_update_tmep.duplicated()].tolist())\n",
    "\n",
    "lines_to_update2.groupby(level=group_name).apply(update_lines_to_delete)\n",
    "\n",
    "# remove lines their geometry not going to change\n",
    "lines_to_update3= lines_to_update2[lines_to_update2[geometry]!=lines_to_update2[new_geometry]]\n",
    "\n",
    "\n",
    "# Save files\n",
    "lines_to_update2['new_geometry'].to_file(f'{intersection_agg_folder}new_geometry.shp')\n",
    "lines_to_update3['new_geometry'].to_file(f'{intersection_agg_folder}new_geometry_1.shp')\n",
    "# For test only\n",
    "network_delete = network[network.index.isin(lines_to_delete)]\n",
    "network_delete.reset_index().to_file(f'{intersection_agg_folder}network_delete.shp')\n",
    "\n",
    "# 4.3 Change the point of each line with new point\n",
    "network_new = network[~network.index.isin(lines_to_delete)]\n",
    "def update_network_with_aggregated_point(group):\n",
    "    lines_in_group = group.explode(index_right)\n",
    "\n",
    "    def update_one_line(points_data):\n",
    "        if points_data.name not in lines_to_delete:\n",
    "            updated_line_geo =network_new.loc[points_data.name]\n",
    "            line_coords = updated_line_geo.geometry.coords\n",
    "            if Point(line_coords[0])==points_data.geometry:\n",
    "                network_new.at[points_data.name,geometry] = LineString([points_data[new_geometry]] + line_coords[1:])\n",
    "            elif Point(line_coords[-1])==points_data.geometry:\n",
    "                network_new.at[points_data.name,geometry] = LineString(line_coords[:-1]+[points_data[new_geometry]])\n",
    "            else:\n",
    "                print(points_data)\n",
    "                print(lines_in_group)\n",
    "    lines_in_group.set_index(index_right).apply(update_one_line,axis=1)\n",
    "\n",
    "lines_to_update3.groupby(level=group_name).apply(update_network_with_aggregated_point)\n",
    "network_new.to_file(f'{intersection_agg_folder}network_new.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# For test\n",
    "get_five_largest_streets(final2.network,'name',f'{data_folder}/delete_2_nodes/test_all_new')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}